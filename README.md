# ggbm
Dream team gradient boosting realization

Планируется написать утилиту для реализации градиентного бустинга на c++. Запуск будет осуществляться в консоли с передачей параметров в command line:
* режим (обучение, применение)
* путь ко входному csv файлу для обучения или применения
* путь к выходному файлу для применения
* параметры для обучения (как минимум количество деревьев, learning rate, thread count, l2-regularization, max depth, возможно другие параметры для регуляризации, планируется поддержать две loss функции MSE и logloss для бинарной классификации)


Планируется многопоточная реализация с использованием гистограммного подхода и oblivious trees.
Минимальный план экспериментов: на датасетах Higgs и BCI с использованием функции потерь binary logloss провести обучение с использованием утилиты, а также фреймворков LightGBM, XGBoost и Catboost последних версий. Список параметров для сравнения:
* Время работы
* Пиковое потребление памяти
* Достигнутое качество на отложенной выборке

Для экспериментов будут задействованы одинаковые наборы гиперпараметров (тк мы реализуем лишь незначительный набор регуляризаций и для сокращения возможного перебора гиперпараметров фреймворков-конкурентов утилиты).
Эксперимент будет состоять в переборе гиперпараметров и последующей оценке качества с помощью кроссвалидации на 70% данных и затем обучения на этих 70% и оценке лучшей полученной в ходе кроссвалидации модели на 30% holdout выборке. Реализация кроссвалидации в утилите пока за рамками проекта.

